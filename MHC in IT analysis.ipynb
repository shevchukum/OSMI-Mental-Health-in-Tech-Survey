{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48aa052c-ed93-4b5e-b997-2189b2e8fb28",
   "metadata": {},
   "source": [
    "# 1. Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800419af-ddff-4897-a2a3-2d38b45941fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "df_2014 = pd.read_csv('2014.csv')\n",
    "df_2016 = pd.read_csv('2016.csv')\n",
    "df_2017 = pd.read_csv('2017.csv')\n",
    "df_2018 = pd.read_csv('2018.csv')\n",
    "df_2019 = pd.read_csv('2019.csv')\n",
    "df_2020 = pd.read_csv('2020.csv')\n",
    "df_2021 = pd.read_csv('2021.csv')\n",
    "df_2022 = pd.read_csv('2022.csv')\n",
    "\n",
    "shapes = []\n",
    "missing = []\n",
    "year = []\n",
    "for y in range(2014, 2023):\n",
    "    if y == 2015: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    year.append(y)\n",
    "    shapes.append(eval(df).shape)\n",
    "    missing.append('{0:.2f}'.format(np.sum(eval(df).isna().sum().tolist()) / eval(df).size))\n",
    "pd.DataFrame({'Year': year, 'Shapes': shapes, 'Missing data share': missing})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8153ad-70f8-4724-90db-65a8bc239b64",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">Note: Section 1.1 \"Scope of the problem\" in the report is done in notebook section 2.3 (below).</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f616582-4ea5-4847-9827-de46fc892823",
   "metadata": {},
   "source": [
    "# 1.2. MHC types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9819e9-8c2a-439d-b5ed-83dd2fe577ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming 2016 open question MHC types to 2017-2022 types\n",
    "\n",
    "mhc_2016_dict =  {'ADD (w/o Hyperactivity)': 'MHC5',\n",
    "                  'Addictive Disorder': 'MHC12',\n",
    "                  'Anxiety Disorder (Generalized, Social, Phobia, etc)': 'MHC1',\n",
    "                   'Asperges': 'MHC6',\n",
    "                   'Attention Deficit Hyperactivity Disorder': 'MHC5',\n",
    "                   'Autism': 'MHC6',\n",
    "                   \"Autism (Asperger's)\": 'MHC6',\n",
    "                   'Autism Spectrum Disorder': 'MHC6',\n",
    "                   'Autism spectrum disorder': 'MHC6',\n",
    "                   'Burn out': 'MHC9',\n",
    "                   'Combination of physical impairment (strongly near-sighted) with a possibly mental one (MCD / \"ADHD\", though its actually a stimulus filtering impairment)': 'MHC1',\n",
    "                   'Depression': 'MHC2',\n",
    "                   'Dissociative Disorder': 'MHC10',\n",
    "                   'Eating Disorder (Anorexia, Bulimia, etc)': 'MHC4',\n",
    "                   'Gender Dysphoria': 'MHC13',\n",
    "                    \"I haven't been formally diagnosed, so I felt uncomfortable answering, but Social Anxiety and Depression.\": np.nan,\n",
    "                    'Intimate Disorder': 'MHC6',\n",
    "                    'Mood Disorder (Depression, Bipolar Disorder, etc)': 'MHC2',\n",
    "                    'Obsessive-Compulsive Disorder': 'MHC7',\n",
    "                    'PDD-NOS': 'MHC13',\n",
    "                    'PTSD (undiagnosed)': 'MHC8',\n",
    "                    'Personality Disorder (Borderline, Antisocial, Paranoid, etc)': 'MHC6',\n",
    "                     'Pervasive Developmental Disorder (Not Otherwise Specified)': 'MHC13',\n",
    "                     'Post-traumatic Stress Disorder': 'MHC8',\n",
    "                     'Psychotic Disorder (Schizophrenia, Schizoaffective, etc)': 'MHC3',\n",
    "                     'Schizotypal Personality Disorder': 'MHC6',\n",
    "                     'Seasonal Affective Disorder': 'MHC2',\n",
    "                     'Sexual addiction': 'MHC12',\n",
    "                     'Sleeping Disorder': 'MHC9',\n",
    "                     'Stress Response Syndromes': 'MHC9',\n",
    "                     'Substance Use Disorder': 'MHC11',\n",
    "                     'Transgender': np.nan,             # not MHC\n",
    "                     'Traumatic Brain Injury': np.nan   # not MHC\n",
    "                 }\n",
    "\n",
    "mhc_dict = {'MHC1': 'Anxiety Disorder (Generalized, Social, Phobia, etc)',\n",
    "             'MHC2': 'Mood Disorder (Depression, Bipolar Disorder, etc)',\n",
    "             'MHC3': 'Psychotic Disorder (Schizophrenia, Schizoaffective, etc)',\n",
    "             'MHC4': 'Eating Disorder (Anorexia, Bulimia, etc)',\n",
    "             'MHC5': 'Attention Deficit Hyperactivity Disorder',\n",
    "             'MHC6': 'Personality Disorder (Borderline, Antisocial, Paranoid, etc)',\n",
    "             'MHC7': 'Obsessive-Compulsive Disorder',\n",
    "             'MHC8': 'Post-Traumatic Stress Disorder',\n",
    "             'MHC9': 'Stress Response Syndromes',\n",
    "             'MHC10': 'Dissociative Disorder',\n",
    "             'MHC11': 'Substance Use Disorder',\n",
    "             'MHC12': 'Addictive Disorder',\n",
    "             'MHC13': 'Other'}\n",
    "\n",
    "# We create 13 features: one fore each MHC type\n",
    "# There is only one open question about MHC types in 2016\n",
    "for index, row in df_2016.iterrows():\n",
    "    if pd.isna(row['If yes, what condition(s) have you been diagnosed with?']):\n",
    "        for key in mhc_dict.keys(): df_2016.at[index, key] = np.nan\n",
    "    else:\n",
    "        mhc_list = row['If yes, what condition(s) have you been diagnosed with?'].split(\"|\")\n",
    "        replace_list = [mhc_2016_dict.get(key, 'Unknown') for key in mhc_list]\n",
    "        for key in mhc_dict.keys():\n",
    "            if key in replace_list:\n",
    "                df_2016.at[index, key] = 1              # MHC is present\n",
    "            else: df_2016.at[index, key] = 0            # MHC is not present\n",
    "\n",
    "\n",
    "# In 2019 there is also only one question for MHC types, but types options are fixed\n",
    "for index, row in df_2019.iterrows():\n",
    "    if pd.isna(row['*If so, what disorder(s) were you diagnosed with?*']):\n",
    "        for key in mhc_dict.keys(): df_2019.at[index, key] = np.nan\n",
    "    else:\n",
    "        elem = row['*If so, what disorder(s) were you diagnosed with?*']\n",
    "        elem = elem.replace('Post-traumatic Stress Disorder', 'Post-Traumatic Stress Disorder')\n",
    "        for mhc in mhc_dict.values():\n",
    "            i = elem.find(mhc)\n",
    "            if i != -1:                                  # MHC found\n",
    "                elem = elem[i + len(elem):]              # Reduce element on the found string\n",
    "                key = next((k for k, v in mhc_dict.items() if v == mhc), None)  # Getting the key value like MHCn for the found value in elem \n",
    "                df_2019.at[index, key] = 1\n",
    "        for k in mhc_dict.keys():\n",
    "            try: \n",
    "                if pd.isna(df_2019.at[index, k]): df_2019.at[index, k] = 0\n",
    "            except KeyError: df_2019.at[index, k] = 0 \n",
    "\n",
    "# Filling in MHC types variables in 2017-2022 waves, except for 2019 \n",
    "for y in range(2017, 2023):\n",
    "    if y == 2019: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    # Fixing mistake in columns naming \n",
    "    eval(df).rename(columns={'Post-traumatic Stress Disorder': 'Post-Traumatic Stress Disorder.1',\n",
    "                             'Post-traumatic Stress Disorder.1': 'Post-Traumatic Stress Disorder.2'}, inplace=True)\n",
    "    # Going row by row, checking 3 questions for each MHC type and filling new MHC features    \n",
    "    for index, row in eval(df).iterrows():\n",
    "        for key in mhc_dict.keys():\n",
    "            if pd.isna(row[mhc_dict[key]]) and pd.isna(row[mhc_dict[key] + '.1']) and pd.isna(row[mhc_dict[key] + '.2']):\n",
    "                eval(df).at[index, key] = 0                 # If all three MHC type variables are nan put into MHCn column zero\n",
    "            else: eval(df).at[index, key] = 1               # If at least one variable is not nan -> put into MHCn column 1\n",
    "        if eval(df).loc[index, 'MHC1':'MHC13'].sum() == 0: \n",
    "            eval(df).loc[index, 'MHC1':'MHC13'] = np.nan    # In case all MHCn's are zero -> replace zeros with nan\n",
    "\n",
    "# Print bar chart for MHC types\n",
    "ws = [0]*13\n",
    "for y in range(2016, 2023):\n",
    "    df = \"df_\" + str(y)\n",
    "    ns = [eval(df)[x].sum() for x in mhc_dict.keys()]\n",
    "    ws = [sum(x) for x in zip(ws, ns)]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "p = plt.bar(mhc_dict.keys(), ws)\n",
    "plt.bar_label(p, label_type='center', color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4614a-1bda-4358-bfaf-d9a707125d5b",
   "metadata": {},
   "source": [
    "# 2.1 Target variables (Section 2 \"Data preparation\" in the report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015a57b-8fa7-4114-819a-3122bdc2a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with MHC tratment effect on productivity and begin collect total_df - final dataset \n",
    "# to be used for modeling\n",
    "\n",
    "ques_dic = {'If you have a mental health issue, do you feel that it interferes with your work when being treated effectively?': 'ProTE',\n",
    "            'If you have a mental health issue, do you feel that it interferes with your work when NOT being treated effectively?': 'ProNTE',\n",
    "            'If you have a mental health disorder, how often do you feel that it interferes with your work <strong>when being treated effectively?</strong>': 'ProTE',\n",
    "            'If you have a mental health disorder, how often do you feel that it interferes with your work <strong>when <em>NOT</em> being treated effectively (i.e., when you are experiencing symptoms)?</strong>': 'ProNTE',\n",
    "            'If you have a mental health disorder, how often do you feel that it interferes with your work *when being treated effectively?*': 'ProTE',\n",
    "            'If you have a mental health disorder, how often do you feel that it interferes with your work *when* _*NOT*_* being treated effectively (i.e., when you are experiencing symptoms)?*': 'ProNTE',\n",
    "            'If you have a mental health disorder, how often do you feel that it interferes with your work <strong>when</strong> <em><strong>NOT</strong></em><strong> being treated effectively (i.e., when you are experiencing symptoms)?</strong>': 'ProNTE'\n",
    "           }\n",
    "\n",
    "enc_dic = {'Never': 1, 'Rarely': 0.95, 'Sometimes': 0.9, 'Often': 0.75, 'Not applicable to me': np.nan}\n",
    "col_list = ['ProTE', 'ProNTE', 'year']\n",
    "total_df = pd.DataFrame(columns=col_list)\n",
    "total_df['year'] = [2014] * 1260\n",
    "total_df['ProTE'] = [np.nan] * 1260\n",
    "total_df['ProNTE'] = [np.nan] * 1260\n",
    "\n",
    "for y in range(2016, 2023):\n",
    "    df = \"df_\" + str(y)\n",
    "    eval(df)['year'] = y\n",
    "    eval(df).rename(columns=ques_dic, inplace=True)\n",
    "    # Concatenate Series from each wave into total_df\n",
    "    total_df = pd.concat([total_df, eval(df)[col_list]], ignore_index=True)\n",
    "for col in col_list: total_df[col].replace(enc_dic, inplace=True)   # Encode ordinal with numerical ones\n",
    "\n",
    "# Creating first target variable ProD = difference of ProNTE and ProTE\n",
    "total_df['ProD'] = total_df['ProTE'] - total_df['ProNTE'] \n",
    "\n",
    "# Let's make a bar chart for negative, zero and positive values of ProD\n",
    "# Calculate mean values for negative, zero and positive values of ProD\n",
    "means = [round(total_df[total_df['ProD']<0]['ProD'].mean(), 2),\n",
    "         round(total_df[total_df['ProD']==0]['ProD'].mean(), 2),\n",
    "         round(total_df[total_df['ProD']>0]['ProD'].mean(), 2)]\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(['Negative tratment effect', 'No effect', 'Positive tratment effect'],\n",
    "            [total_df[total_df['ProD']<0]['ProD'].count(), \n",
    "             total_df[total_df['ProD']==0]['ProD'].count(), \n",
    "             total_df[total_df['ProD']>0]['ProD'].count()])\n",
    "ax.bar_label(bars, label_type='center', color='white')\n",
    "i = 0\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.5, 'Mean:' + str(means[i]), ha='center', va='bottom')\n",
    "    i += 1\n",
    "ax.set_ylabel('Mean Value')\n",
    "ax.set_title('Figure 2. ProD frequency and means for negative, zero and positive effect of tratment')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c2e07-c7dc-479f-82d2-cd89f5e3fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second target variable ST - if the person sought treatment\n",
    "ques_dic = {'Have you sought treatment for a mental health condition?': 'ST',                                      # 2014\n",
    "            'Have you ever sought treatment for a mental health issue from a mental health professional?': 'ST',   # 2016\n",
    "            'Have you ever sought treatment for a mental health disorder from a mental health professional?': 'ST' # 2017-2022\n",
    "           }\n",
    "\n",
    "enc_dic = {'Yes': 1, 'No': 0}\n",
    "ts = pd.Series()\n",
    "for y in range(2014, 2023):\n",
    "    if y == 2015: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    eval(df).rename(columns=ques_dic, inplace=True)\n",
    "    # Concatenate Series from each wave into total_df\n",
    "    ts = pd.concat([ts, eval(df)['ST']], ignore_index=True)\n",
    "total_df['ST'] = ts\n",
    "total_df['ST'].replace(enc_dic, inplace=True)   # Encode with numerical values\n",
    "\n",
    "# Plotting the bar plot\n",
    "years = total_df['ST'].groupby(total_df.year).value_counts(normalize=True).xs(1, level='ST').index\n",
    "values = total_df['ST'].groupby(total_df.year).value_counts(normalize=True).xs(1, level='ST').values\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(years, values)\n",
    "ax.set_xticks(years)\n",
    "ax.set_xticklabels(years)\n",
    "ax.bar_label(bars, labels=[f'{v:.2f}' for v in values], label_type='center', color='white')\n",
    "ax.set_ylabel('Proportion of people seeking treatment ST=1')\n",
    "ax.set_title('Figure 3. Trend of the seeking tratment share ST=1.')\n",
    "plt.show()\n",
    "\n",
    "# Chi-Square test if ST is independent from year\n",
    "contingency_table = pd.crosstab(total_df['year'], total_df['ST'])\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "print('Chi-Square test if ST is independent from year')\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe80a90-6cb1-42eb-ad35-fa4587dace21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHCA - if the person has MHC and \n",
    "# MHCD if a person has MHC with official diagnosis (1 if yes, 0 if no)\n",
    "\n",
    "ques_dic = {'Do you currently have a mental health disorder?': 'DIS',\n",
    "            'Do you *currently* have a mental health disorder?': 'DIS'\n",
    "           }\n",
    "\n",
    "ts = pd.Series([np.nan] * 1260) \n",
    "for y in range(2016, 2023):\n",
    "    df = \"df_\" + str(y)\n",
    "    eval(df).rename(columns=ques_dic, inplace=True)\n",
    "    ts = pd.concat([ts, eval(df)['DIS']], ignore_index=True)\n",
    "\n",
    "def encoder(dis):\n",
    "    if pd.isna(dis): return np.nan\n",
    "    elif dis == 'Yes': return 1\n",
    "    elif dis in ['Maybe', 'Possibly']: return 0\n",
    "    else: return np.nan\n",
    "\n",
    "def encoder2(dis):\n",
    "    if pd.isna(dis): return np.nan\n",
    "    elif dis in ['Yes', 'Maybe', 'Possibly']: return 1\n",
    "    elif dis == 'No': return 0\n",
    "    else: return np.nan\n",
    "\n",
    "total_df['MHCD'] = ts.apply(encoder)\n",
    "total_df['MHCA'] = ts.apply(encoder2)\n",
    "\n",
    "# Checking share of people having MHC with diagnosis in 2016\n",
    "diag = 'Have you been diagnosed with a mental health condition by a medical professional?'\n",
    "values = [df_2016[df_2016.DIS == 'Yes'][diag].value_counts(normalize=True).sort_index()[1],\n",
    "          df_2016[df_2016.DIS == 'Maybe'][diag].value_counts(normalize=True).sort_index()[1],\n",
    "          df_2016[df_2016.DIS == 'No'][diag].value_counts(normalize=True).sort_index()[1]]\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(['Yes', 'Maybe', 'No'], values)\n",
    "ax.bar_label(bars, labels=[f'{v:.2f}' for v in values], label_type='center', color='white')\n",
    "ax.set_ylabel('Proportion of people seeking treatment ST=1')\n",
    "ax.set_xlabel(\"Different answers on 'DIS' question\")\n",
    "ax.set_title('Figure 4. Share of people with and without MHC which have been diagnosed by a medical professional.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297621b2-f58f-477b-81ca-45e567bb90dc",
   "metadata": {},
   "source": [
    "# 2.2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8bdf25-383c-4a4b-b4cc-47afec076299",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">2.2.1. Demographics</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff4756-06c2-4b44-89a2-8143f4bc9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shere of people working and living in different countries is very small\n",
    "print('Table 4. Shere of people working and living in different countries.')\n",
    "shares = []\n",
    "year = []\n",
    "for y in range(2016, 2023):\n",
    "    df = \"df_\" + str(y)\n",
    "    year.append(y)\n",
    "    a = {}\n",
    "    for col in eval(df).columns:\n",
    "        if 'country' in str.lower(col):\n",
    "            if len(a) == 0: \n",
    "                a = pd.DataFrame(eval(df)[col].value_counts())\n",
    "            else: \n",
    "                b = pd.DataFrame(eval(df)[col].value_counts())\n",
    "                c = a.join(b, how='left', rsuffix='_work', lsuffix='_live')\n",
    "                # Count people with not the same work and live country, divide by two to avoid double counting\n",
    "                c['diff'] = np.abs(c.count_live - c.count_work) / 2          \n",
    "                shares.append('{0:.3f}'.format(c['diff'].sum() / c.count_live.sum()))\n",
    "\n",
    "pd.DataFrame({'Year': year, 'Shares': shares})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e350843-3cc5-4cb8-a42b-364c8098a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additing country CTRY to total_df\n",
    "ques_dic = {'Country': 'CTRY',\n",
    "            'What country do you live in?': 'CTRY',\n",
    "            'What country do you *live* in?': 'CTRY',\n",
    "            'What country do you <strong>live</strong> in?': 'CTRY'}\n",
    "\n",
    "ts = pd.Series()\n",
    "for y in range(2014, 2023):\n",
    "    if y == 2015: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    eval(df).rename(columns=ques_dic, inplace=True)\n",
    "    ts = pd.concat([ts, eval(df)['CTRY']], ignore_index=True)\n",
    "\n",
    "ts.fillna('United States', inplace=True)      # Fill nan country (2 samples) with the most frequent value - United States\n",
    "ts.replace({'United States of America': 'United States'}, inplace=True)\n",
    "ts = ts.apply(lambda x: 'Other' if x not in ['United States', 'United Kingdom', 'Canada', 'Germany'] else x)\n",
    "total_df['CTRY'] = ts\n",
    "total_df['CTRY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc7fbf-4f6b-41bd-aa60-dd543edd10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017['What is your race?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0a56a-489a-4076-b66e-0cc23b7e5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking age \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "age = pd.Series()\n",
    "for y in range(2014, 2023):\n",
    "    if y == 2015: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    for col in eval(df).columns:\n",
    "        if col == 'Age' or col == 'What is your age?':\n",
    "            age = pd.concat([age, eval(df)[col]], ignore_index=True)\n",
    "\n",
    "age = age.apply(lambda x: np.nan if x > 75 else x)\n",
    "age = age.apply(lambda x: np.nan if x < 18 else x)\n",
    "print('Missing:', age.isna().sum())\n",
    "age.fillna(value=age.median(), inplace=True)\n",
    "total_df['Age'] = age\n",
    "print('Age median:', age.median())\n",
    "\n",
    "plt.boxplot(age)\n",
    "plt.title('Figure 5. Age boxplot for 8 survey waves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aedab9-c4dd-461e-9a78-6f885645a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender if messy in 2016\n",
    "plt.figure(figsize=(20,5))\n",
    "df_2016['What is your gender?'].value_counts().plot(kind='bar', color = 'C0')\n",
    "print('Number of unique genders:', len(df_2016['What is your gender?'].value_counts().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f08a1b-a679-4ad4-bbc9-f5df6331ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Gender variable in all waves\n",
    "male_list = ['M', 'Male', 'male', 'm', 'Male-ish', 'maile', 'Cis Male', 'Mal', 'Male (CIS)', 'Make', 'Guy (-ish) ^_^',\n",
    "            'Male ', 'Man', 'msle', 'Female (trans)', 'Mail', 'cis male', 'Malr', 'Cis Man', 'man', 'Cis male', 'Male.', \n",
    "            'Male (cis)', 'Sex is male', 'Dude', \"I'm a man why didn't you make this a drop down question. You should of asked sex? And I would of answered yes please. Seriously how much text can this take? \",\n",
    "            'mail', 'M|', 'male ', 'cisdude', 'cis man', 'MALE', 'cis-male', 'cis hetero male', \"male (hey this is the tech industry you're talking about)\",\n",
    "            'Cis-male', 'Male, cis', 'cis male ', 'dude', 'male, born with xy chromosoms', 'Malel', 'Cisgender male', 'Masculine', 'I have a penis',\n",
    "            'masculino', 'CIS Male', 'mostly male', 'cisgender male', 'MAle', 'male/he/him', 'homem cis', 'cis-het male', 'varón']\n",
    "female_list = ['Female', 'female', 'Cis Female', 'F', 'Woman', 'f', 'Femake', 'woman', 'Female ', 'cis-female/femme', 'Female (cis)', 'femail',\n",
    "              'female ', 'Female assigned at birth ', 'fm', 'Cis female ', 'female/woman', 'Cisgender Female', 'fem', \n",
    "              'Female (props for making this a freeform field, though)', ' Female', 'Cis-woman', 'femalw', 'female (cis)', 'My sex is female.',\n",
    "              'female (cisgender)', 'Female (cis) ', 'cis-Female', 'cis female', 'F, cisgender', '*shrug emoji* (F)', 'Cis woman', \n",
    "              'Female (cisgender)', 'Cis-Female', 'Cisgendered woman', 'cisgender female', 'cis woman', 'femmina', 'Femile', 'FEMALE', 'female, she/her',\n",
    "              'Female, cis-gendered']\n",
    "\n",
    "gender = pd.Series()\n",
    "for y in range(2014, 2023):\n",
    "    if y == 2015: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    for col in eval(df).columns:\n",
    "        if col == 'Gender' or col == 'What is your gender?':\n",
    "            gender = pd.concat([gender, eval(df)[col]], ignore_index=True)\n",
    "   \n",
    "gender = gender.replace(male_list, 'Male')\n",
    "gender = gender.replace(female_list, 'Female')\n",
    "print(\"Value counts of Gender before filling nan's:\")\n",
    "print(gender.value_counts(normalize=True))\n",
    "\n",
    "# Replace nan's (34 observations) randomly\n",
    "categories = ['Male', 'Female', 'Other']\n",
    "probabilities = [0.715534, 0.251012, 1 - 0.715534 - 0.251012]\n",
    "gender = gender.apply(lambda x: np.random.choice(categories, p=probabilities) if pd.isna(x) else x)\n",
    "gender = gender.apply(lambda x: 'Other' if x not in ['Male', 'Female', 'Other'] else x)\n",
    "print('')\n",
    "print(\"Value counts of Gender after filling nan's:\")\n",
    "print(gender.value_counts())\n",
    "total_df['Gender'] = gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225fbf1-a556-44ce-891a-8f6ff66af81d",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">2.2.2. Employer MHC friendliness features</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7914b2-4805-4683-81ed-8ec975d289e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemblying intervention features\n",
    "\n",
    "ques_dic = {'---2014--------------------------------------------------------------------------------------------------': 0,\n",
    "            'Has your employer ever discussed mental health as part of an employee wellness program?': 'INT1_1',\n",
    "            'Does your employer provide resources to learn more about mental health issues and how to seek help?': 'INT1_2',\n",
    "            'Do you know the options for mental health care your employer provides?': 'INT1_3',\n",
    "            'Does your employer provide mental health benefits?': 'INT2_1',\n",
    "            'Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?': 'INT3_1',\n",
    "            'How easy is it for you to take medical leave for a mental health condition?': 'INT3_2',\n",
    "            'Would you be willing to discuss a mental health issue with your coworkers?': 'INT3_3',\n",
    "            'Would you be willing to discuss a mental health issue with your direct supervisor(s)?': 'INT3_4',\n",
    "            '----2016---------------------------------------------------------------------------------------------------': 0,\n",
    "            'Has your employer ever formally discussed mental health (for example, as part of a wellness campaign or other official communication)?': 'INT1_1',\n",
    "            'Does your employer offer resources to learn more about mental health concerns and options for seeking help?': 'INT1_2',\n",
    "            'Do you know the options for mental health care available under your employer-provided coverage?': 'INT1_3',\n",
    "            'Does your employer provide mental health benefits as part of healthcare coverage?': 'INT2_1',            \n",
    "            'Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources provided by your employer?': 'INT3_1',\n",
    "            'If a mental health issue prompted you to request a medical leave from work, asking for that leave would be:': 'INT3_2',\n",
    "            'Would you feel comfortable discussing a mental health disorder with your coworkers?': 'INT3_3',\n",
    "            'Would you feel comfortable discussing a mental health disorder with your direct supervisor(s)?': 'INT3_4',\n",
    "            '----2017-2022------------------------------------------------------------------------------------------------': 0,\n",
    "            'Does your employer offer resources to learn more about mental health disorders and options for seeking help?': 'INT1_2',\n",
    "            'Do you know the options for mental health care available under your employer-provided health coverage?': 'INT1_3',\n",
    "            'Does your employer provide mental health benefits\\xa0as part of healthcare coverage?': 'INT2_1',\n",
    "            'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?': 'INT3_2',\n",
    "            'Would you feel comfortable discussing a mental health issue with your coworkers?': 'INT3_3',\n",
    "            'Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?': 'INT3_4'\n",
    "           }\n",
    "\n",
    "enc_dic_1 =  {'No': False, 'Yes': True, \"Don't know\": False, \"I don't know\": False}                                             #INT1 and 3_1\n",
    "enc_dic_2 =  { 'No': False, 'Yes': True, \"Don't know\": np.nan, \"I don't know\": np.nan,\n",
    "             'Not eligible for coverage / N/A': False, 'Not eligible for coverage / NA': False                                   #INT2\n",
    "             }                                \n",
    "enc_dic_3_2 = {'Very difficult': 0, 'Difficult': 1/6, 'Somewhat difficult': 1/3, 'Neither easy nor difficult': 1/2, \n",
    "             'Somewhat easy': 3/4, 'Very easy': 1, \"I don't know\": np.nan, \"Don't know\" : np.nan                                 #INT3_2\n",
    "            }\n",
    "enc_dic_3_3 = {'No': 0, 'Yes': 1, \"Maybe\": 0.5, \"Some of them\": 0.25}                                                             #NT3_3, 3_4                                                                                #NT3_3, 3_4 \n",
    "\n",
    "#, 'Not sure': False, 'I am not sure': Fals\n",
    "\n",
    "# Raplacing column names\n",
    "new_cols = ['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']\n",
    "wdf = pd.DataFrame(columns=new_cols)\n",
    "for y in range(2014, 2023):\n",
    "    if y == 2015: continue\n",
    "    df = \"df_\" + str(y)\n",
    "    eval(df).rename(columns=ques_dic, inplace=True)\n",
    "    wdf = pd.concat([wdf, eval(df)[new_cols]], ignore_index=True)\n",
    "\n",
    "\n",
    "# Encoding\n",
    "for col in new_cols:                                    \n",
    "    if col == 'INT2_1':\n",
    "        wdf[col].replace(enc_dic_2, inplace=True)\n",
    "    elif col == 'INT3_2':\n",
    "        wdf[col].replace(enc_dic_3_2, inplace=True)\n",
    "    if col in ['INT3_3', 'INT3_4']:\n",
    "        wdf[col].replace(enc_dic_3_3, inplace=True)                                       # Fill with numerical values\n",
    "    else:\n",
    "        wdf[col].replace(enc_dic_1, inplace=True)                                         # Fill INT1 and 3_1 with bool values\n",
    "\n",
    "# Creating new information variable based on INT2_1 Don't know answer\n",
    "wdf['INT1_4'] = wdf['INT2_1'].apply(lambda x: True if x in [True, False] else False)\n",
    "\n",
    "# Checking number of nan's\n",
    "print('Missing values:')\n",
    "print(wdf.isna().sum(), wdf.shape[0])\n",
    "\n",
    "# Dropping rows with all nan's\n",
    "for index, row in wdf.iterrows():\n",
    "    if row.isna().sum() > 6: wdf.drop(index, inplace=True)\n",
    "print('')\n",
    "print(\"Missing values after droping lives with all nan's:\")\n",
    "print(wdf.isna().sum(), wdf.shape[0])\n",
    "print('')\n",
    "\n",
    "# Print correlation matrix\n",
    "correlation_matrix = wdf.corr(method='spearman')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Figure 6. Rang correlation matrix of MHC friendliness features (Spearman method).')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdc7de-70c4-4b1e-885b-ebc3a39a2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing with linear regrssion predictions for INT3_2\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.tools.tools as sm\n",
    "\n",
    "cols = ['INT1_1', 'INT1_2', 'INT3_1', 'INT3_3', 'INT3_4']\n",
    "X = wdf[~pd.isna(wdf['INT3_2'])][cols]\n",
    "X_train = sm.add_constant(X)\n",
    "y = wdf[~pd.isna(wdf['INT3_2'])].INT3_2\n",
    "nan_indices = wdf[wdf['INT3_2'].isna()].index\n",
    "\n",
    "# Using EFS for choosing best features for linera regression of INT2_1 on othe INT's\n",
    "model = LinearRegression()\n",
    "efs = EFS(estimator=model,       \n",
    "         min_features=1,     \n",
    "         max_features=5,     \n",
    "         scoring='neg_mean_squared_error', \n",
    "         cv=4)\n",
    "\n",
    "efs = efs.fit(X_train, y)\n",
    "print('Best features combination:', efs.best_feature_names_)\n",
    "\n",
    "# Get predicitons and replace nan's in INT3_2\n",
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "model.fit(X_train_best, y)\n",
    "X_pred = wdf[pd.isna(wdf['INT3_2'])][list(efs.best_feature_names_)]\n",
    "y_pred = model.predict(X_pred)\n",
    "wdf.loc[nan_indices, 'INT3_2'] = y_pred\n",
    "wdf.INT3_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25988b78-044d-4dd3-9812-6fa295b1164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing with logit regrssion predictions for INT2_1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cols = ['INT1_1', 'INT1_2', 'INT3_1', 'INT3_3', 'INT3_4']\n",
    "X = wdf[~pd.isna(wdf['INT2_1'])][cols]\n",
    "X_train = sm.add_constant(X)\n",
    "y = wdf[~pd.isna(wdf['INT2_1'])].INT2_1.astype(int)\n",
    "nan_indices = wdf[wdf['INT2_1'].isna()].index\n",
    "\n",
    "# Using EFS for choosing best features for linera regression of INT2_1 on othe INT's\n",
    "model = LogisticRegression()\n",
    "efs = EFS(estimator=model,       \n",
    "         min_features=1,     \n",
    "         max_features=5,     \n",
    "         scoring='accuracy', \n",
    "         cv=4)\n",
    "\n",
    "efs = efs.fit(X_train, y)\n",
    "print('Best features combination:', efs.best_feature_names_)\n",
    "\n",
    "# Get predicitons and replace nan's in INT3_2\n",
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "model.fit(X_train_best, y)\n",
    "X_pred = wdf[pd.isna(wdf['INT2_1'])][list(efs.best_feature_names_)]\n",
    "y_pred = model.predict(X_pred)\n",
    "wdf.loc[nan_indices, 'INT2_1'] = y_pred\n",
    "wdf['INT2_1'] = wdf.INT2_1.astype(bool)\n",
    "# Check that there is no nan's in wdf\n",
    "print(wdf.isna().sum(), wdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d1fbc-b7f6-458b-b6dc-1b776f7f9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.join(wdf)\n",
    "print(total_df.isna().sum(), total_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2bbdb-304f-4491-b127-f209fd1ee5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA analysis for INT features\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(wdf)        # Standartization -> mean =0, std = 1\n",
    "df_standardized = pd.DataFrame(df_standardized, columns=wdf.columns)\n",
    "\n",
    "pca = PCA(n_components=3)                             \n",
    "pca_transformed = pca.fit_transform(df_standardized)\n",
    "pca_df = pd.DataFrame(pca_transformed, columns=['PC1', 'PC2', 'PC3'])\n",
    "combined_df = pd.concat([df_standardized, pca_df], axis=1)\n",
    "\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print('Cummulative variance by component:', cum_var_exp)\n",
    "\n",
    "# Compute the correlation matrix and plot the heatmap of the correlation matrix\n",
    "correlation_matrix = combined_df.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Figure 7. Correlation matrix between original INT features and PCA components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06a75b-35fe-4cda-bbc4-10b2fff5923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot with MDS1, MDS2, and color representing MDS3\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(combined_df['PC1'], combined_df['PC2'], c=combined_df['PC3'], cmap='viridis', s=100, edgecolor='k') \n",
    "# Add a color bar\n",
    "plt.colorbar(scatter, label='PC3')\n",
    "# Add labels and title\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Figure 8. Scatter Plot of PC1 vs PC2 with PC3 being the color.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ac999-803e-4232-9dc4-b8fe4271b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try MDS\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "original_distances = euclidean_distances(wdf)\n",
    "\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "mds_transformed = mds.fit_transform(wdf)\n",
    "mds_distances = euclidean_distances(mds_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9ec4b-496e-4756-812e-7bf528e6a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calulate explained variation:\n",
    "stress = mds.stress_\n",
    "explained_variation = 1 - (stress / np.sum(original_distances**2))\n",
    "print(f\"Explained Variation: {explained_variation}\")\n",
    "\n",
    "# Create a DataFrame for the MDS output\n",
    "mds_df = pd.DataFrame(mds_transformed, columns=['MDS1', 'MDS2'])\n",
    "combined_df = pd.concat([wdf, mds_df], axis=1)\n",
    "\n",
    "# Compute the and plot correlation matrix\n",
    "correlation_matrix = combined_df.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Figure 9. Correlation Matrix between Original Features and MDS Dimensions.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65258f-1d8a-42ab-8a14-a5b05b1ca72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot with MDS1, MDS2, and color representing MDS3\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(mds_df['MDS1'], mds_df['MDS2'], s=100, edgecolor='k')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('MDS1')\n",
    "plt.ylabel('MDS2')\n",
    "plt.title('Figure 10. Scatter Plot of MDS1 vs MDS2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b66d6-40da-46dd-bf17-a3421be3cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try DBSCAN to find some clusters\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(wdf)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=2, min_samples=200)  # You might need to adjust these parameters\n",
    "clusters = dbscan.fit_predict(df_standardized)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "wdf['Cluster'] = clusters\n",
    "\n",
    "# Check the unique clusters\n",
    "print(\"Number of clusters found:\", len(set(clusters)) - (1 if -1 in clusters else 0))  # -1 is the noise/outliers\n",
    "wdf.Cluster.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43dd5c-e05f-4bec-a532-a7b8604fb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating MHC friendliness index\n",
    "wdf['Index'] = (wdf.INT1_1 + wdf.INT1_2 + wdf.INT1_4 + wdf.INT2_1 + wdf.INT3_1 + wdf.INT3_2 + wdf.INT3_3 + wdf.INT3_4)/8 \n",
    "\n",
    "# Define marker styles for each cluster\n",
    "marker_dict = {0:'o', 1:'s', 2:'D', 3:'>', 4:'<'}  # Different shapes (circles, squares, diamonds, etc.)\n",
    "\n",
    "# Plot each cluster with a different marker\n",
    "plt.figure(figsize=(8, 6))\n",
    "clus = [0,1,2,3]\n",
    "for cluster in clus:\n",
    "    cluster_data = wdf[wdf['Cluster'] == cluster]\n",
    "    random = np.random.rand(cluster_data.shape[0])\n",
    "    scatter = plt.scatter(\n",
    "                cluster_data['Index'], random,\n",
    "                label=f'Cluster {cluster}', \n",
    "                marker=marker_dict[cluster], \n",
    "                s=20\n",
    "                )\n",
    "\n",
    "plt.title('Figure 11. DBSCAN Clustering')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Random numbers from 0 to 1')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aae792-1222-44d4-81bf-cd0b87ba67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if cluters contain companies of different size\n",
    "\n",
    "# Obtaining company size from 2016-2022 waves\n",
    "ts = pd.Series([np.nan]*1260)\n",
    "for y in range(2016, 2023):\n",
    "    df = \"df_\" + str(y)\n",
    "    eval(df).rename(columns={'How many employees does your company or organization have?': 'CSIZE'}, inplace=True)\n",
    "    ts = pd.concat([ts, eval(df).CSIZE], ignore_index=True)\n",
    "wdf['CSIZE'] = ts\n",
    "\n",
    "# Decrease the number of sizes\n",
    "def resize(size):\n",
    "    if pd.isna(size): return np.nan\n",
    "    elif size in ['1-5', '6-25', '26-100']: return '0-100'\n",
    "    elif size in ['100-500', '500-1000']: return '100-1000'\n",
    "    else: return 'More than 1000'\n",
    "\n",
    "wdf['CSIZE'] = wdf['CSIZE'].apply(resize)\n",
    "csize_categories = ['0-100', '100-1000', 'More than 1000']\n",
    "\n",
    "tdf = pd.DataFrame(index=clus)\n",
    "for cluster in clus:\n",
    "    freq_dif = wdf[wdf.Cluster == cluster].CSIZE.value_counts(normalize=True) - wdf.CSIZE.value_counts(normalize=True)\n",
    "    for csize in csize_categories:\n",
    "        tdf.at[cluster, csize] = freq_dif.xs(csize)\n",
    "\n",
    "# Plotting the grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Define the positions of the bars\n",
    "bar_width = 0.2\n",
    "positions = np.arange(len(clus))\n",
    "\n",
    "# Plot each category and add labels\n",
    "bars = []\n",
    "for i, category in enumerate(csize_categories):\n",
    "    bar = ax.bar(positions + i * bar_width, tdf[category], width=bar_width, label=category)\n",
    "    bars.append(bar)\n",
    "    # Adding labels to each bar\n",
    "    ax.bar_label(bar, labels=[f'{v:.2f}' for v in tdf[category]], padding=3, fontsize=10)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Normalized Difference')\n",
    "ax.set_title('Figure 12. Company size structures for different clusters relative to average structure.')\n",
    "ax.set_xticks(positions + bar_width)  # Adjust x-ticks to align with clusters\n",
    "ax.set_xticklabels(clus)\n",
    "ax.legend(title='CSIZE Categories')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb40e8d-3c60-489a-b467-f2fb9f19b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = total_df.join(wdf.Cluster)\n",
    "tdf = pd.DataFrame(index=clus)\n",
    "for cluster in clus:\n",
    "    freq_dif = tds[tds.Cluster == cluster].CTRY.value_counts(normalize=True) - tds.CTRY.value_counts(normalize=True)\n",
    "    for ctry in tds.CTRY.unique():\n",
    "        tdf.at[cluster, ctry] = freq_dif.xs(ctry)\n",
    "\n",
    "# Plotting the grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the positions of the bars\n",
    "bar_width = 0.15\n",
    "positions = np.arange(len(clus))\n",
    "\n",
    "# Plot each category and add labels\n",
    "bars = []\n",
    "for i, category in enumerate(tds.CTRY.unique()):\n",
    "    bar = ax.bar(positions + i * bar_width, tdf[category], width=bar_width, label=category)\n",
    "    bars.append(bar)\n",
    "    # Adding labels to each bar\n",
    "    ax.bar_label(bar, labels=[f'{v:.2f}' for v in tdf[category]], padding=3, fontsize=10)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Normalized Difference')\n",
    "ax.set_title('Figure 13. Country structures for different clusters relative to average structure.')\n",
    "ax.set_xticks(positions + bar_width)  # Adjust x-ticks to align with clusters\n",
    "ax.set_xticklabels(clus)\n",
    "ax.legend(title='Countries')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b99d9f-08b1-437d-b051-8637cd4aeaaf",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">2.3 Section \"Introduction\" in the report</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2518f5-515b-41e8-a500-113a7c0d5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add Index column to total_df\n",
    "total_df = total_df.join(wdf.Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6387c6d-3ea4-47fa-9290-a75ba3a6a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have all variables needed to make scope of the problme calculations\n",
    "# In 2016-2022 waves 40% of people who answered a question 'Do you currently have a mental health disorder?' (or similar), answered ‘Yes’\n",
    "# and another 26% ‘Maybe’ (or similar)\n",
    "print(df_2016.DIS.value_counts(normalize=True))\n",
    "\n",
    "# If we encode ProNTE (and ProTE) as [0.75, 0.9, 0.95, 1] we get average ProNTE in our dataset for people with diagnosis 0.79, \n",
    "# which is 21% productivity loss. Treated MHC, according to this dataset, leads to a 9% reduction in productivity on average.\n",
    "print('')\n",
    "print('Average productivity of people with untreated diagnosed MHC:', total_df[total_df.MHCD == 1].ProNTE.mean()) \n",
    "print('')\n",
    "print('Average productivity of people with treated diagnosed MHC:', total_df[total_df.MHCD == 1].ProTE.mean())\n",
    "\n",
    "# For people without diagnosis average productivity loss is 16% if untreated and 9% if treated, \n",
    "# hence productivity gain from treatment is lower - only 7%.\n",
    "print('')\n",
    "print('Average productivity of people with untreated not diagnosed MHC:', total_df[total_df.MHCD == 0].ProNTE.mean()) \n",
    "print('')\n",
    "print('Average productivity of people with treated not diagnosed MHC:', total_df[total_df.MHCD == 0].ProTE.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905d8b5-548c-4eec-b85f-f22c77f5b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how MHC friendliness imapct people willingness to seek tratment\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "tdf = total_df[['Index', 'MHCD', 'ST']].copy()\n",
    "\n",
    "BM1 = (tdf['MHCD'] == 1) & (tdf.Index < 0.25)\n",
    "BM2 = (tdf['MHCD'] == 1) & (tdf.Index > 0.75)\n",
    "BM3 = (tdf['MHCD'] == 0) & (tdf.Index < 0.25)\n",
    "BM4 = (tdf['MHCD'] == 0) & (tdf.Index > 0.75)\n",
    "\n",
    "a1 = tdf[BM1].ST.astype(int)\n",
    "a2 = tdf[BM2].ST.astype(int)\n",
    "\n",
    "b1 = tdf[BM3].ST.dropna().astype(int)\n",
    "b2 = tdf[BM4].ST.dropna().astype(int)\n",
    "\n",
    "print('People with diag seeking Treatment in MHC not friendly companies:', a1.mean(), a1.shape)\n",
    "print('People with diag seeking Treatment in MHC friendly companies:', a2.mean(), a2.shape)\n",
    "\n",
    "print('People with no diag seeking Treatment in MHC not friendly companies:', b1.mean(), b1.shape)\n",
    "print('People with no diag seeking Treatment in MHC friendly companies:', b2.mean(), b2.shape)\n",
    "\n",
    "# Welch's t-test \n",
    "t_stat, p_value = ttest_ind(a1,a2, equal_var=False)\n",
    "print(f\"T-statistic for DIAG people: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "t_stat, p_value = ttest_ind(b1,b2, equal_var=False)\n",
    "print(f\"T-statistic for no DIAG people: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7a586-1781-4d1c-bfa4-9487e3054038",
   "metadata": {},
   "source": [
    "# 3.1. Modeing MHCA and MHCD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3ba26-40a5-47b6-b8d6-0efea063272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking independence of MHCA and MHCD from our features.\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "wdf = total_df.copy()\n",
    "wdf = wdf.join(pd.get_dummies(total_df[['CTRY', 'Gender']], drop_first=True))\n",
    "wdf.drop(columns=['Gender', 'CTRY', 'ProTE', 'ProNTE', 'year', 'ProD', 'ST', 'MHCD', 'Index'], inplace=True)\n",
    "wdf.drop(columns=['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4'], inplace=True)\n",
    "wdf = wdf.dropna()\n",
    "\n",
    "X = wdf.drop(columns=['MHCA'])\n",
    "y = wdf['MHCA']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Select the best features\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# print Chi²-statistics- and p-values per feature\n",
    "tdf = pd.DataFrame({'features': X.columns.values, 'Scores': selector.scores_, 'p-values': selector.pvalues_}).sort_values(by='p-values')\n",
    "tdf[tdf['p-values'] <= 0.05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e47ab-59cc-4ad4-8c4b-c48a2796824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using EFS with Logistic Regression to get the best features to maximize accuracy\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Add a constant to the independent variables matrix (required for statsmodels)\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Select the best features to max accuracy\n",
    "model_lr = LogisticRegression(max_iter=10000)\n",
    "efs = EFS(estimator=model_lr,       \n",
    "         min_features=1,     \n",
    "         max_features=8,     \n",
    "         scoring='neg_log_loss', \n",
    "         cv=4)\n",
    "\n",
    "efs = efs.fit(X_train, y_train)\n",
    "print('Best features combination:', efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e095bf7-8e66-431a-be60-0f56cc545dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best features and evaluate on the test dataset \n",
    "import copy\n",
    "\n",
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "X_test_best = X_test[list(efs.best_feature_names_)]\n",
    "\n",
    "model_lr.fit(X_train_best, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred_lr = model_lr.predict(X_test_best)\n",
    "y_pred_proba_lr = model_lr.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "# Calculate accuracy, AUC score and log loss\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "ll_score = log_loss(y_test, y_pred_proba_lr)\n",
    "print(f\"Log Loss: {ll_score:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= confusion_matrix(y_test, y_pred_lr))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "if 'const' not in efs.best_feature_names_: \n",
    "    X_ols = X[list(efs.best_feature_names_)]\n",
    "else: \n",
    "    X_ols = X[list(efs.best_feature_names_[1:])]\n",
    "    X_ols = sm.add_constant(X_ols)\n",
    "\n",
    "model = sm.Logit(y, X_ols.astype(float))  # Create a Logit model\n",
    "result = model.fit() \n",
    "print(result.summary())\n",
    "\n",
    "# Saving the model for deployement\n",
    "model_mhca = copy.deepcopy(model_lr)\n",
    "columns_mhca = copy.deepcopy(efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac05d6-102e-4131-a7b4-b209cc56a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating calibration coefficient for MHCA for this dataset and general USA population\n",
    "# Result is 0.708 / 0.228 = 3.105\n",
    "# Note that MHCA prediciton for this dataset 0.6578 is close to the true value 0.645628\n",
    "\n",
    "calibration_point = {'const': [1.0, 1.0],\n",
    "                     'Age': [39.0, wdf['Age'].median()],\n",
    "                     'CTRY_Germany': [0, wdf.CTRY_Germany.mean()],\n",
    "                     'CTRY_United Kingdom': [0, wdf['CTRY_United Kingdom'].mean()],\n",
    "                     'CTRY_United States': [1.0, wdf['CTRY_United States'].mean()],\n",
    "                     'Gender_Male': [0.495, wdf.Gender_Male.mean()],\n",
    "                     'Gender_Other': [0.0036, wdf.Gender_Other.mean()]}\n",
    "cal_df = pd.DataFrame(calibration_point)\n",
    "# Predict probabilities\n",
    "print('Best Logit model prediction of MHCA for USA general poluation:', model_lr.predict_proba(cal_df)[:, 1][0])\n",
    "print('Best Logit model prediction of MHCA for our dataset:', model_lr.predict_proba(cal_df)[:, 1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e4196-e43f-49d5-945f-f754c29ebe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build XG Boost Classifier we first divide training set on small training and validation \n",
    "X_small_train, X_val, y_small_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "columns_to_drop = ['CTRY_United Kingdom']\n",
    "X_small_train.drop(columns=columns_to_drop, inplace=True)\n",
    "X_val.drop(columns=columns_to_drop, inplace=True) \n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.015, 0.02],\n",
    "    'subsample': [0.75, 0.8, 0.85]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_log_loss', cv=3, verbose=1)\n",
    "\n",
    "grid_search.fit(X_small_train, y_small_train)\n",
    "\n",
    "# Get the best params and model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use permutations to get feature importance\n",
    "feat_imp = permutation_importance(best_model, X_val, y_val, n_repeats=10, scoring='neg_log_loss')\n",
    "pd.DataFrame({'features': X_val.columns, 'importances_mean': feat_imp['importances_mean'],\n",
    "              'importances_std': feat_imp['importances_std']}).sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0bb3b-8bb4-430e-a566-ae5b7203ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_xg = best_model.predict(X_test.drop(columns=columns_to_drop))\n",
    "y_pred_proba_xg = best_model.predict_proba(X_test.drop(columns=columns_to_drop))[:, 1]\n",
    "\n",
    "# Evaluate the accuracy, AUC scrore and Log Loss\n",
    "accuracy = accuracy_score(y_test, y_pred_xg)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_xg)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "ll_score = log_loss(y_test, y_pred_proba_xg)\n",
    "print(f\"Log Loss: {ll_score:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= confusion_matrix(y_test, y_pred_xg))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea2caa-b9e2-415a-a538-bca2bc7095a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking calibration coefficient is we use XGBoost for predictions\n",
    "# Result is 0.75136 / 0.228 = 3.30. Avergae of this coeff and 3.105 we got from Logit is 3.20. \n",
    "# Note that MHCA prediciton for this dataset 0.63118 is close to the true value 0.645628\n",
    "\n",
    "calibration_point = {'const': [1.0, 1.0],\n",
    "                     'Age': [39.0, wdf['Age'].median()],\n",
    "                     'CTRY_Germany': [0, wdf['CTRY_Germany'].mean()],\n",
    "                     'CTRY_Other': [0, wdf.CTRY_Other.mean()],\n",
    "                     'CTRY_United States': [1.0, wdf['CTRY_United States'].mean()],\n",
    "                     'Gender_Male': [0.495, wdf.Gender_Male.mean()],\n",
    "                     'Gender_Other': [0.0036, wdf.Gender_Other.mean()]}\n",
    "cal_df = pd.DataFrame(calibration_point)\n",
    "# Predict probabilities\n",
    "print('Best XGBoost model prediction of MHCA for USA general poluation:', best_model.predict_proba(cal_df)[:, 1][0])\n",
    "print('Best XGBoost model prediction of MHCA for our dataset:', best_model.predict_proba(cal_df)[:, 1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b7c3d-2265-49cc-8ff8-3d156bde895e",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">Modeling MHCD</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7242b0-c717-46a8-8849-44d63c7d7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To model MHCD we filter out people with MHC == 0\n",
    "wdf = total_df[total_df.MHCA == 1].copy()\n",
    "wdf = wdf.join(pd.get_dummies(total_df[['CTRY', 'Gender']], drop_first=True))\n",
    "wdf = wdf.drop(columns=['Gender', 'CTRY', 'ProTE', 'ProNTE', 'year', 'ProD', 'ST', 'MHCA', 'Index']).dropna()\n",
    "wdf.drop(columns=['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4'], inplace=True)\n",
    "wdf = wdf.dropna()\n",
    "\n",
    "X = wdf.drop(columns=['MHCD'])\n",
    "y = wdf['MHCD']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Select the features\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# print Chi²-statistics- and p-values per feature\n",
    "tdf = pd.DataFrame({'features': X.columns.values, 'Scores': selector.scores_, 'p-values': selector.pvalues_}).sort_values(by='p-values')\n",
    "tdf[tdf['p-values'] <= 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581c859-0dcc-45c9-a73e-b9d5dc470ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using EFS with Logistic Regression to get the best features to maximize accuracy\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Add a constant to the independent variables matrix (required for statsmodels)\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Select the best features to max accuracy\n",
    "model_lr = LogisticRegression(max_iter=10000)\n",
    "efs = EFS(estimator=model_lr,       \n",
    "         min_features=1,     \n",
    "         max_features=8,     \n",
    "         scoring='neg_log_loss', \n",
    "         cv=4)\n",
    "\n",
    "efs = efs.fit(X_train, y_train)\n",
    "print('Best features combination:', efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef10b7-63c0-4d2e-9a94-e4119cbc90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best features and evaluate on the test dataset \n",
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "X_test_best = X_test[list(efs.best_feature_names_)]\n",
    "\n",
    "model_lr.fit(X_train_best, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred_lr = model_lr.predict(X_test_best)\n",
    "y_pred_proba_lr = model_lr.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "# Calculate accuracy, AUC score and log loss\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "ll_score = log_loss(y_test, y_pred_proba_lr)\n",
    "print(f\"Log Loss: {ll_score:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= confusion_matrix(y_test, y_pred_lr))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "if 'const' not in efs.best_feature_names_: \n",
    "    X_ols = X[list(efs.best_feature_names_)]\n",
    "else: \n",
    "    X_ols = X[list(efs.best_feature_names_[1:])]\n",
    "    X_ols = sm.add_constant(X_ols)\n",
    "\n",
    "model = sm.Logit(y, X_ols.astype(float))  # Create a Logit model\n",
    "result = model.fit() \n",
    "print(result.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da0cc8-9c29-4942-b67a-b030c11105bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build XG Boost Classifier we first divide training set on small training and validation \n",
    "X_small_train, X_val, y_small_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "columns_to_drop = ['CTRY_Other', 'CTRY_Germany', 'Age', 'CTRY_United Kingdom']\n",
    "X_small_train.drop(columns=columns_to_drop, inplace=True)\n",
    "X_val.drop(columns=columns_to_drop, inplace=True) \n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.015, 0.02],\n",
    "    'subsample': [0.55, 0.6, 0.65]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_log_loss', cv=3, verbose=1)\n",
    "\n",
    "grid_search.fit(X_small_train, y_small_train)\n",
    "\n",
    "# Get the best params and model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use permutations to get feature importance\n",
    "feat_imp = permutation_importance(best_model, X_val, y_val, n_repeats=10, scoring='neg_log_loss')\n",
    "pd.DataFrame({'features': X_val.columns, 'importances_mean': feat_imp['importances_mean'],\n",
    "              'importances_std': feat_imp['importances_std']}).sort_values(by='importances_mean', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004ae60-fc81-484d-8fbf-330c72539fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_xg = best_model.predict(X_test.drop(columns=columns_to_drop))\n",
    "y_pred_proba_xg = best_model.predict_proba(X_test.drop(columns=columns_to_drop))[:, 1]\n",
    "\n",
    "# Evaluate the accuracy, AUC scrore and Log Loss\n",
    "accuracy = accuracy_score(y_test, y_pred_xg)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_xg)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "ll_score = log_loss(y_test, y_pred_proba_xg)\n",
    "print(f\"Log Loss: {ll_score:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= confusion_matrix(y_test, y_pred_xg))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Saving the model for deployement\n",
    "model_mhcd = copy.deepcopy(best_model)\n",
    "columns_mhcd = copy.deepcopy(X_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f190d9-8cd2-4c0e-9ee6-221bf022ccbd",
   "metadata": {},
   "source": [
    "# 3.2. Modeling ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038aadf-9d88-44b4-a3c1-44d0e5b5f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST = 1 if the person sought tratment from MHC\n",
    "# All INT features in the model\n",
    "\n",
    "wdf = total_df[total_df.MHCA == 1].copy()\n",
    "wdf = wdf.join(pd.get_dummies(total_df[['CTRY', 'Gender']], drop_first=True))\n",
    "wdf = wdf.drop(columns=['Gender', 'CTRY', 'ProTE', 'ProNTE', 'year', 'ProD', 'MHCA', 'Index']).dropna()\n",
    "wdf[['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']] = \\\n",
    "    wdf[['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']].astype(bool)\n",
    "wdf = wdf.dropna()\n",
    "\n",
    "X = wdf.drop(columns=['ST'])\n",
    "y = wdf['ST'].astype(bool)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# print Chi²-statistics- and p-values per feature\n",
    "tdf = pd.DataFrame({'features': X.columns.values, 'Scores': selector.scores_, 'p-values': selector.pvalues_}).sort_values(by='p-values')\n",
    "tdf[tdf['p-values'] <= 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098e46e-32e7-4751-b9bc-40581160943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the independent variables matrix (required for statsmodels)\n",
    "X_train = sm.add_constant(X_train.astype(float))\n",
    "X_test = sm.add_constant(X_test.astype(float))\n",
    "\n",
    "# Build the logistic regression model\n",
    "logit_model = sm.Logit(y_train, X_train)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Print the summary of the model (includes coefficients, p-values, R-squared, etc.)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee9d4f-5425-4c8e-908d-844688dbab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using EFS with Logistic Regression to get the best features to maximize accuracy\n",
    "X_train_efs = X_train[['const', 'MHCD', 'INT3_3', 'CTRY_United States', 'Gender_Male', 'INT3_2', 'INT1_1', 'INT2_1']]\n",
    "\n",
    "# Select the best features to max accuracy\n",
    "model_lr = LogisticRegression(max_iter=10000)\n",
    "efs = EFS(estimator=model_lr,       \n",
    "         min_features=1,     \n",
    "         max_features=8,     \n",
    "         scoring='neg_log_loss', \n",
    "         cv=4)\n",
    "\n",
    "efs = efs.fit(X_train_efs, y_train)\n",
    "print('Best features combination:', efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2310e-a50a-48a8-a9d8-12c435f2ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "X_test_best = X_test[list(efs.best_feature_names_)]\n",
    "\n",
    "model_lr.fit(X_train_best, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred_lr = model_lr.predict(X_test_best)\n",
    "y_pred_proba_lr = model_lr.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "# Calculate accuracy, AUC score and log loss\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "ll_score = log_loss(y_test, y_pred_proba_lr)\n",
    "print(f\"Log Loss: {ll_score:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= confusion_matrix(y_test, y_pred_lr))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "if 'const' not in efs.best_feature_names_: \n",
    "    X_ols = X[list(efs.best_feature_names_)]\n",
    "else: \n",
    "    X_ols = X[list(efs.best_feature_names_[1:])]\n",
    "    X_ols = sm.add_constant(X_ols)\n",
    "\n",
    "model = sm.Logit(y, X_ols.astype(float))  # Create a Logit model\n",
    "result = model.fit() \n",
    "print(result.summary())\n",
    "\n",
    "# Saving the model for deployement\n",
    "model_st = copy.deepcopy(model_lr)\n",
    "columns_st = copy.deepcopy(efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aae01c-bab7-4f44-8c4b-762c3b46e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build XG Boost Classifier we first divide training set on small training and validation \n",
    "# Split the training set into validation and small training sets\n",
    "X_small_train, X_val, y_small_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "columns_to_drop = ['CTRY_United Kingdom', 'INT3_1', 'INT3_4', 'Gender_Other']\n",
    "X_small_train.drop(columns=columns_to_drop, inplace=True)\n",
    "X_val.drop(columns=columns_to_drop, inplace=True) \n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.015, 0.02, 0.25],\n",
    "    'subsample': [0.55, 0.6, 0.65]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_log_loss', cv=3, verbose=1)\n",
    "\n",
    "grid_search.fit(X_small_train, y_small_train)\n",
    "\n",
    "# Get the best params and model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use permutations to get feature importance\n",
    "feat_imp = permutation_importance(best_model, X_val, y_val, n_repeats=10, scoring='neg_log_loss')\n",
    "pd.DataFrame({'features': X_val.columns, 'importances_mean': feat_imp['importances_mean'],\n",
    "              'importances_std': feat_imp['importances_std']}).sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752450e-9026-47b6-ad3f-ea985e0cd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_xg = best_model.predict(X_test.drop(columns=columns_to_drop))\n",
    "y_pred_proba_xg = best_model.predict_proba(X_test.drop(columns=columns_to_drop))[:, 1]\n",
    "\n",
    "# Evaluate the accuracy, AUC scrore and Log Loss\n",
    "accuracy = accuracy_score(y_test, y_pred_xg)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_xg)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "ll_score = log_loss(y_test, y_pred_proba_xg)\n",
    "print(f\"Log Loss: {ll_score:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix= confusion_matrix(y_test, y_pred_xg))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660111dd-dd66-419e-82dd-b48df1afbc62",
   "metadata": {},
   "source": [
    "# 3.3. Modeling treatment effect on productivity - ProD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da331a5-0547-4922-8fd8-c53c309e6e0d",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">3.3.1. Productivity model for people without diagnosis</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "682266a0-6d96-49e7-ac7f-9c57fd8630f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>Scores</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gender_Other</td>\n",
       "      <td>3.1205</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        features  Scores  p-values\n",
       "14  Gender_Other  3.1205  0.002211"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "wdf = total_df[total_df.MHCA == 1].copy()\n",
    "wdf = wdf.join(pd.get_dummies(total_df[['CTRY', 'Gender']], drop_first=True))\n",
    "wdf = wdf.drop(columns=['Gender', 'CTRY', 'ProTE', 'ProNTE', 'year', 'MHCA', 'Index']).dropna()\n",
    "wdf = wdf.dropna()\n",
    "wdf[['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']] = \\\n",
    "    wdf[['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']].astype(bool)\n",
    "\n",
    "# We start with people without diagnosis and who sought treatment\n",
    "wdf = wdf[(wdf.MHCD == 0) & (wdf.ST == 1)]\n",
    "wdf.drop(columns=['MHCD', 'ST'], inplace=True)\n",
    "\n",
    "X = wdf.drop(columns=['ProD'])\n",
    "y = wdf['ProD']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "selector.fit_transform(X, y)\n",
    "\n",
    "# print ANOVA-statistics- and p-values per feature\n",
    "tdf = pd.DataFrame({\n",
    "    'features': X.columns,  \n",
    "    'Scores': selector.scores_,\n",
    "    'p-values': selector.pvalues_\n",
    "})\n",
    "\n",
    "significant_features = tdf.sort_values(by='p-values')\n",
    "significant_features[significant_features['p-values'] <= 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "bfaf834c-5800-4ec7-ac13-51cdebdf5d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.00647366445439573\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   ProD   R-squared:                       0.085\n",
      "Model:                            OLS   Adj. R-squared:                  0.014\n",
      "Method:                 Least Squares   F-statistic:                     1.200\n",
      "Date:                Wed, 21 Aug 2024   Prob (F-statistic):              0.274\n",
      "Time:                        21:31:13   Log-Likelihood:                 240.68\n",
      "No. Observations:                 211   AIC:                            -449.4\n",
      "Df Residuals:                     195   BIC:                            -395.7\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   0.1194      0.048      2.492      0.014       0.025       0.214\n",
      "Age                    -0.0005      0.001     -0.733      0.465      -0.002       0.001\n",
      "INT1_1                  0.0004      0.016      0.023      0.982      -0.032       0.032\n",
      "INT1_2                 -0.0137      0.016     -0.887      0.376      -0.044       0.017\n",
      "INT2_1                  0.0067      0.015      0.452      0.652      -0.023       0.036\n",
      "INT3_1                  0.0274      0.013      2.058      0.041       0.001       0.054\n",
      "INT3_2                 -0.0149      0.028     -0.524      0.601      -0.071       0.041\n",
      "INT3_3                  0.0286      0.014      2.113      0.036       0.002       0.055\n",
      "INT3_4                 -0.0008      0.013     -0.059      0.953      -0.027       0.025\n",
      "INT1_4                  0.0109      0.014      0.766      0.444      -0.017       0.039\n",
      "CTRY_Germany           -0.0577      0.041     -1.396      0.164      -0.139       0.024\n",
      "CTRY_Other             -0.0381      0.032     -1.186      0.237      -0.101       0.025\n",
      "CTRY_United Kingdom    -0.0223      0.034     -0.662      0.509      -0.089       0.044\n",
      "CTRY_United States     -0.0407      0.028     -1.454      0.148      -0.096       0.015\n",
      "Gender_Male            -0.0017      0.013     -0.137      0.891      -0.027       0.023\n",
      "Gender_Other           -0.0043      0.036     -0.120      0.905      -0.075       0.066\n",
      "==============================================================================\n",
      "Omnibus:                       59.297   Durbin-Watson:                   1.740\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               12.480\n",
      "Skew:                           0.234   Prob(JB):                      0.00195\n",
      "Kurtosis:                       1.904   Cond. No.                         449.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Add a constant to the independent variables matrix (required for statsmodels)\n",
    "X_train = sm.add_constant(X_train.astype(float))\n",
    "X_test = sm.add_constant(X_test.astype(float))\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "713d48e5-917a-4868-aa4c-fac3b309f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 127/127"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features combination: ('INT3_3', 'INT1_4')\n"
     ]
    }
   ],
   "source": [
    "# Select the best features to max accuracy\n",
    "model_ols = LinearRegression()\n",
    "efs = EFS(estimator=model_ols,       \n",
    "         min_features=1,     \n",
    "         max_features=7,     \n",
    "         scoring='neg_mean_squared_error', \n",
    "         cv=4)\n",
    "\n",
    "X_train_ols = X_train[['const', 'INT3_3', 'INT2_1', 'INT1_4', 'CTRY_United States', 'Gender_Male', 'Gender_Other']]\n",
    "efs = efs.fit(X_train_ols, y_train)\n",
    "print('Best features combination:', efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "6351f595-d25c-4f18-ae0b-4c9e62cb54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.006417231713006256\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                   ProD   R-squared (uncentered):                   0.496\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.492\n",
      "Method:                 Least Squares   F-statistic:                              129.0\n",
      "Date:                Wed, 21 Aug 2024   Prob (F-statistic):                    1.00e-39\n",
      "Time:                        21:31:21   Log-Likelihood:                          283.10\n",
      "No. Observations:                 264   AIC:                                     -562.2\n",
      "Df Residuals:                     262   BIC:                                     -555.0\n",
      "Df Model:                           2                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "INT3_3         0.0526      0.009      5.666      0.000       0.034       0.071\n",
      "INT1_4         0.0512      0.009      5.656      0.000       0.033       0.069\n",
      "==============================================================================\n",
      "Omnibus:                       17.411   Durbin-Watson:                   1.840\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.186\n",
      "Skew:                           0.322   Prob(JB):                      0.00614\n",
      "Kurtosis:                       2.285   Cond. No.                         2.66\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Train the model with best features and evaluate on the test dataset \n",
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "X_test_best = X_test[list(efs.best_feature_names_)]\n",
    "\n",
    "model_ols.fit(X_train_best, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred_ols = model_ols.predict(X_test_best)\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred_ols)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "if 'const' not in efs.best_feature_names_: \n",
    "    X_ols = X[list(efs.best_feature_names_)]\n",
    "else: \n",
    "    X_ols = X[list(efs.best_feature_names_[1:])]\n",
    "    X_ols = sm.add_constant(X_ols)\n",
    "\n",
    "result = sm.OLS(y, X_ols).fit()\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Saving the model for deployement\n",
    "model_ProNDia = copy.deepcopy(model_ols)\n",
    "columns_ProNDia = copy.deepcopy(efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "25b9eb9a-144f-438d-b653-44a840e5d950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "Best parameters found:  {'learning_rate': 0.025, 'max_depth': 3, 'n_estimators': 20, 'subsample': 0.65}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importances_mean</th>\n",
       "      <th>importances_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INT1_4</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTRY_United States</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTRY_United Kingdom</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importances_mean  importances_std\n",
       "1               INT1_4          0.000297         0.000161\n",
       "3   CTRY_United States          0.000241         0.000135\n",
       "2  CTRY_United Kingdom          0.000100         0.000025\n",
       "0                const          0.000000         0.000000"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To build XG Boost Classifier we first divide training set on small training and validation \n",
    "# Split the training set into validation and small training sets\n",
    "X_small_train, X_val, y_small_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "columns_to_drop = ['CTRY_Germany', 'Age', 'Gender_Male', 'INT3_4', 'INT1_1', 'INT2_1', 'INT3_1', \n",
    "                   'CTRY_Other', 'INT3_2', 'INT1_2', 'INT3_3', 'Gender_Other']\n",
    "X_small_train.drop(columns=columns_to_drop, inplace=True)\n",
    "X_val.drop(columns=columns_to_drop, inplace=True) \n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.02, 0.025, 0.03],\n",
    "    'subsample': [0.6, 0.65, 0.7]\n",
    "}\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=3, verbose=1)\n",
    "\n",
    "grid_search.fit(X_small_train, y_small_train)\n",
    "\n",
    "# Get the best params and model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use permutations to get feature importance\n",
    "feat_imp = permutation_importance(best_model, X_val, y_val, n_repeats=10, scoring='neg_mean_squared_error')\n",
    "pd.DataFrame({'features': X_val.columns, 'importances_mean': feat_imp['importances_mean'],\n",
    "              'importances_std': feat_imp['importances_std']}).sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "7f50fc3d-59ec-4c96-a71c-dab5cbec0081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0060938169607535515\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_xg = best_model.predict(X_test.drop(columns=columns_to_drop))\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred_xg)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111af680-b747-4f45-b395-2461bb2be1ba",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:0.7em;\">3.3.2. Productivity model for people with diagnosis</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "ad635daf-eeed-4006-bb4d-3124e4bfc7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>Scores</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [features, Scores, p-values]\n",
       "Index: []"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf = total_df[total_df.MHCA == 1].copy()\n",
    "wdf = wdf.join(pd.get_dummies(total_df[['CTRY', 'Gender']], drop_first=True))\n",
    "wdf = wdf.drop(columns=['Gender', 'CTRY', 'ProTE', 'ProNTE', 'year', 'MHCA', 'Index']).dropna()\n",
    "wdf = wdf.dropna()\n",
    "wdf[['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']] = \\\n",
    "    wdf[['INT1_1', 'INT1_2', 'INT1_4', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4']].astype(bool)\n",
    "\n",
    "# We start with people without diagnosis and who sought treatment\n",
    "wdf = wdf[(wdf.MHCD == 1) & (wdf.ST == 1)]\n",
    "wdf.drop(columns=['MHCD', 'ST'], inplace=True)\n",
    "\n",
    "X = wdf.drop(columns=['ProD'])\n",
    "y = wdf['ProD']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "selector.fit_transform(X, y)\n",
    "\n",
    "# print ANOVA-statistics- and p-values per feature\n",
    "tdf = pd.DataFrame({\n",
    "    'features': X.columns,  \n",
    "    'Scores': selector.scores_,\n",
    "    'p-values': selector.pvalues_\n",
    "})\n",
    "\n",
    "significant_features = tdf.sort_values(by='p-values')\n",
    "significant_features[significant_features['p-values'] <= 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "bd9027c7-3a01-42ee-a050-193b6726eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.005626456078027124\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   ProD   R-squared:                       0.025\n",
      "Model:                            OLS   Adj. R-squared:                  0.007\n",
      "Method:                 Least Squares   F-statistic:                     1.398\n",
      "Date:                Wed, 21 Aug 2024   Prob (F-statistic):              0.141\n",
      "Time:                        20:02:44   Log-Likelihood:                 929.69\n",
      "No. Observations:                 819   AIC:                            -1827.\n",
      "Df Residuals:                     803   BIC:                            -1752.\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   0.1112      0.025      4.468      0.000       0.062       0.160\n",
      "Age                    -0.0005      0.000     -1.459      0.145      -0.001       0.000\n",
      "INT1_1                  0.0007      0.007      0.105      0.916      -0.013       0.015\n",
      "INT1_2                  0.0028      0.007      0.387      0.699      -0.011       0.017\n",
      "INT2_1                 -0.0077      0.008     -0.977      0.329      -0.023       0.008\n",
      "INT3_1                 -0.0021      0.006     -0.343      0.732      -0.014       0.010\n",
      "INT3_2                  0.0166      0.013      1.243      0.214      -0.010       0.043\n",
      "INT3_3                  0.0100      0.007      1.352      0.177      -0.005       0.025\n",
      "INT3_4                 -0.0001      0.007     -0.016      0.987      -0.014       0.014\n",
      "INT1_4                  0.0110      0.008      1.292      0.197      -0.006       0.028\n",
      "CTRY_Germany            0.0334      0.026      1.290      0.197      -0.017       0.084\n",
      "CTRY_Other             -0.0108      0.019     -0.576      0.565      -0.047       0.026\n",
      "CTRY_United Kingdom    -0.0083      0.020     -0.424      0.672      -0.047       0.030\n",
      "CTRY_United States      0.0055      0.017      0.326      0.745      -0.028       0.039\n",
      "Gender_Male            -0.0055      0.006     -0.917      0.360      -0.017       0.006\n",
      "Gender_Other            0.0161      0.012      1.318      0.188      -0.008       0.040\n",
      "==============================================================================\n",
      "Omnibus:                       65.533   Durbin-Watson:                   1.892\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               79.744\n",
      "Skew:                          -0.757   Prob(JB):                     4.83e-18\n",
      "Kurtosis:                       3.206   Cond. No.                         508.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Add a constant to the independent variables matrix (required for statsmodels)\n",
    "X_train = sm.add_constant(X_train.astype(float))\n",
    "X_test = sm.add_constant(X_test.astype(float))\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "14c63fb7-429d-40f5-9153-7a4162f96704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 255/255"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features combination: ('CTRY_United States', 'INT3_3', 'INT1_4', 'CTRY_Germany', 'Age')\n"
     ]
    }
   ],
   "source": [
    "# Select the best features to max accuracy\n",
    "model_ols = LinearRegression()\n",
    "efs = EFS(estimator=model_ols,       \n",
    "         min_features=1,     \n",
    "         max_features=8,     \n",
    "         scoring='neg_mean_squared_error', \n",
    "         cv=4)\n",
    "\n",
    "X_train_ols = X_train[['Gender_Male', 'CTRY_United States', 'Gender_Other', 'INT3_1', 'INT3_3', 'INT1_4', 'CTRY_Germany', 'Age']]\n",
    "efs = efs.fit(X_train_ols, y_train)\n",
    "print('Best features combination:', efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "92300387-3798-4a0f-8925-077bc38ae953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.005563915666854951\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                   ProD   R-squared (uncentered):                   0.695\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.694\n",
      "Method:                 Least Squares   F-statistic:                              464.7\n",
      "Date:                Wed, 21 Aug 2024   Prob (F-statistic):                   6.94e-260\n",
      "Time:                        20:03:06   Log-Likelihood:                          1133.3\n",
      "No. Observations:                1024   AIC:                                     -2257.\n",
      "Df Residuals:                    1019   BIC:                                     -2232.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "CTRY_United States     0.0326      0.006      5.481      0.000       0.021       0.044\n",
      "INT3_3                 0.0236      0.006      4.222      0.000       0.013       0.035\n",
      "INT1_4                 0.0317      0.006      5.026      0.000       0.019       0.044\n",
      "CTRY_Germany           0.0716      0.021      3.469      0.001       0.031       0.112\n",
      "Age                    0.0014      0.000      6.857      0.000       0.001       0.002\n",
      "==============================================================================\n",
      "Omnibus:                       54.177   Durbin-Watson:                   2.013\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               61.430\n",
      "Skew:                          -0.592   Prob(JB):                     4.58e-14\n",
      "Kurtosis:                       2.802   Cond. No.                         291.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Train the model with best features and evaluate on the test dataset \n",
    "X_train_best = X_train[list(efs.best_feature_names_)]\n",
    "X_test_best = X_test[list(efs.best_feature_names_)]\n",
    "\n",
    "model_ols.fit(X_train_best, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred_ols = model_ols.predict(X_test_best)\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred_ols)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "if 'const' not in efs.best_feature_names_: \n",
    "    X_ols = X[list(efs.best_feature_names_)]\n",
    "else: \n",
    "    X_ols = X[list(efs.best_feature_names_[1:])]\n",
    "    X_ols = sm.add_constant(X_ols)\n",
    "\n",
    "result = sm.OLS(y, X_ols.astype(float)).fit()\n",
    "print(result.summary())\n",
    "\n",
    "# Saving the model for deployement\n",
    "model_ProDia = copy.deepcopy(model_ols)\n",
    "columns_ProDia = copy.deepcopy(efs.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa023a6-5f45-4205-846a-c3b5fbf15f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build XG Boost Classifier we first divide training set on small training and validation \n",
    "# Split the training set into validation and small training sets\n",
    "X_small_train, X_val, y_small_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "columns_to_drop = ['INT3_2', 'INT3_1', 'INT1_2', 'Gender_Other', 'CTRY_United States', 'CTRY_United Kingdom', \n",
    "                  'INT3_3', 'const', 'INT1_1', 'INT3_4']\n",
    "X_small_train.drop(columns=columns_to_drop, inplace=True)\n",
    "X_val.drop(columns=columns_to_drop, inplace=True) \n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [4, 5, 6],\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.01, 0.015, 0.02],\n",
    "    'subsample': [0.55, 0.6, 0.65]\n",
    "}\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=3, verbose=1)\n",
    "\n",
    "grid_search.fit(X_small_train, y_small_train)\n",
    "\n",
    "# Get the best params and model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use permutations to get feature importance\n",
    "feat_imp = permutation_importance(best_model, X_val, y_val, n_repeats=10, scoring='neg_mean_squared_error')\n",
    "pd.DataFrame({'features': X_val.columns, 'importances_mean': feat_imp['importances_mean'],\n",
    "              'importances_std': feat_imp['importances_std']}).sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c755d-b6b2-440f-823f-65303eacff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_xg = best_model.predict(X_test.drop(columns=columns_to_drop))\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred_xg)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2b448-22a7-41f0-9b75-538d0c7bfb4f",
   "metadata": {},
   "source": [
    "# 4. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "8a4fb4a8-9d30-4048-963f-0760feb01188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MHC level</th>\n",
       "      <td>0.154176</td>\n",
       "      <td>0.154176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diagnosis level</th>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaking streatment with D</th>\n",
       "      <td>0.877894</td>\n",
       "      <td>0.91378</td>\n",
       "      <td>0.035887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaking streatment without D</th>\n",
       "      <td>0.421881</td>\n",
       "      <td>0.518242</td>\n",
       "      <td>0.096361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Productivity gain from T with D</th>\n",
       "      <td>0.154239</td>\n",
       "      <td>0.155908</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Productivity gain from T without D</th>\n",
       "      <td>0.075697</td>\n",
       "      <td>0.079482</td>\n",
       "      <td>0.003785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total prod gain from T</th>\n",
       "      <td>0.014506</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Start       End      Diff\n",
       "MHC level                           0.154176  0.154176       0.0\n",
       "Diagnosis level                     0.600659  0.600659       0.0\n",
       "Seaking streatment with D           0.877894   0.91378  0.035887\n",
       "Seaking streatment without D        0.421881  0.518242  0.096361\n",
       "Productivity gain from T with D     0.154239  0.155908  0.001669\n",
       "Productivity gain from T without D  0.075697  0.079482  0.003785\n",
       "Total prod gain from T              0.014506  0.015729  0.001224"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimating Case 1\n",
    "\n",
    "total_df[['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4', 'INT1_4']] = \\\n",
    "        total_df[['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4', 'INT1_4']].astype(float)\n",
    "\n",
    "MHCpro_dict = {'const': [1.0, 1.0],\n",
    "              'Age': [33.0, 33.0],\n",
    "              'CTRY_Germany': [1, 1],\n",
    "              'CTRY_United Kingdom': [0, 0],\n",
    "              'CTRY_United States': [0, 0],\n",
    "              'CTRY_Other': [0, 0],\n",
    "              'Gender_Male': [0.7, 0.7],\n",
    "              'Gender_Other': [0.01, 0.01],\n",
    "              'INT1_1': [total_df[total_df.CTRY == 'Germany']['INT1_1'].mean(), 0.9],\n",
    "              'INT1_2': [total_df[total_df.CTRY == 'Germany']['INT1_2'].mean(), 0.9],\n",
    "              'INT2_1': [total_df[total_df.CTRY == 'Germany']['INT2_1'].mean(), 0.9],\n",
    "              'INT3_1': [total_df[total_df.CTRY == 'Germany']['INT3_1'].mean(), 0.5],\n",
    "              'INT3_2': [total_df[total_df.CTRY == 'Germany']['INT3_2'].mean(), 0.67],\n",
    "              'INT3_3': [total_df[total_df.CTRY == 'Germany']['INT3_3'].mean(), 0.5],\n",
    "              'INT3_4': [total_df[total_df.CTRY == 'Germany']['INT3_4'].mean(), 0.7],\n",
    "              'INT1_4': [total_df[total_df.CTRY == 'Germany']['INT1_4'].mean(), 0.8] \n",
    "             }\n",
    "MHCpro_df = pd.DataFrame(MHCpro_dict)\n",
    "\n",
    "result_df = pd.DataFrame(columns=['Start', 'End', 'Diff'], \n",
    "                         index=['MHC level', 'Diagnosis level', 'Seaking streatment with D', 'Seaking streatment without D',\n",
    "                                'Productivity gain from T with D', 'Productivity gain from T without D', 'Total prod gain from T'])\n",
    "\n",
    "# Predicting MHCA change\n",
    "sdf = MHCpro_df[[*columns_mhca]]\n",
    "result_df.at['MHC level', 'Start'] = model_mhca.predict_proba(sdf)[:, 1][0] / 3.2     # Corecting survey self-selection bias on 3.2\n",
    "result_df.at['MHC level', 'End'] = model_mhca.predict_proba(sdf)[:, 1][1] / 3.2\n",
    "\n",
    "# Predicting MHCD change\n",
    "sdf = MHCpro_df[[*columns_mhcd]]\n",
    "result_df.at['Diagnosis level', 'Start'] = model_mhcd.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Diagnosis level', 'End'] = model_mhcd.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# Predicting ST change\n",
    "# for people with diagnosis\n",
    "MHCpro_df['MHCD'] = [1, 1]\n",
    "sdf = MHCpro_df[[*columns_st]]\n",
    "result_df.at['Seaking streatment with D', 'Start'] = model_st.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Seaking streatment with D', 'End'] = model_st.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# for people without diagnosis\n",
    "MHCpro_df['MHCD'] = [0, 0]\n",
    "sdf = MHCpro_df[[*columns_st]]\n",
    "result_df.at['Seaking streatment without D', 'Start'] = model_st.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Seaking streatment without D', 'End'] = model_st.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# Predicting ProD\n",
    "# for people with diagnosis\n",
    "MHCpro_df['MHCD'] = [1, 1]\n",
    "sdf = MHCpro_df[[*columns_ProDia]]\n",
    "result_df.at['Productivity gain from T with D', 'Start'] = model_ProDia.predict(sdf)[0] \n",
    "result_df.at['Productivity gain from T with D', 'End'] = model_ProDia.predict(sdf)[1]\n",
    "\n",
    "# for people without diagnosis\n",
    "MHCpro_df['MHCD'] = [0, 0]\n",
    "sdf = MHCpro_df[[*columns_ProNDia]]\n",
    "result_df.at['Productivity gain from T without D', 'Start'] = model_ProNDia.predict(sdf)[0] \n",
    "result_df.at['Productivity gain from T without D', 'End'] = model_ProNDia.predict(sdf)[1]\n",
    "\n",
    "# productivity gain for people with diagnosis\n",
    "result_df.at['Total prod gain from T', 'Start'] =  result_df.at['MHC level', 'Start'] * \\\n",
    "            (result_df.at['Diagnosis level', 'Start'] * result_df.at['Seaking streatment with D', 'Start'] * \\\n",
    "             result_df.at['Productivity gain from T with D', 'Start'] + \\\n",
    "             (1 - result_df.at['Diagnosis level', 'Start']) * result_df.at['Seaking streatment without D', 'Start'] *\\\n",
    "              result_df.at['Productivity gain from T without D', 'Start'])\n",
    "                                                           \n",
    "result_df.at['Total prod gain from T', 'End'] =  result_df.at['MHC level', 'End'] * \\\n",
    "            (result_df.at['Diagnosis level', 'End'] * result_df.at['Seaking streatment with D', 'End'] * \\\n",
    "             result_df.at['Productivity gain from T with D', 'End'] + \\\n",
    "             (1 - result_df.at['Diagnosis level', 'End']) * result_df.at['Seaking streatment without D', 'End'] *\\\n",
    "              result_df.at['Productivity gain from T without D', 'End'])\n",
    "\n",
    "result_df['Diff'] = result_df['End'] - result_df['Start']\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "ba948bb5-707d-4add-ba1c-0f200cd65aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MHC level</th>\n",
       "      <td>0.154176</td>\n",
       "      <td>0.154176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diagnosis level</th>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaking streatment with D</th>\n",
       "      <td>0.877894</td>\n",
       "      <td>0.902586</td>\n",
       "      <td>0.024693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaking streatment without D</th>\n",
       "      <td>0.421881</td>\n",
       "      <td>0.484656</td>\n",
       "      <td>0.062776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Productivity gain from T with D</th>\n",
       "      <td>0.154239</td>\n",
       "      <td>0.158085</td>\n",
       "      <td>0.003845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Productivity gain from T without D</th>\n",
       "      <td>0.075697</td>\n",
       "      <td>0.08598</td>\n",
       "      <td>0.010283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total prod gain from T</th>\n",
       "      <td>0.014506</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0.001274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Start       End      Diff\n",
       "MHC level                           0.154176  0.154176       0.0\n",
       "Diagnosis level                     0.600659  0.600659       0.0\n",
       "Seaking streatment with D           0.877894  0.902586  0.024693\n",
       "Seaking streatment without D        0.421881  0.484656  0.062776\n",
       "Productivity gain from T with D     0.154239  0.158085  0.003845\n",
       "Productivity gain from T without D  0.075697   0.08598  0.010283\n",
       "Total prod gain from T              0.014506  0.015779  0.001274"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df[['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4', 'INT1_4']] = \\\n",
    "        total_df[['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4', 'INT1_4']].astype(float)\n",
    "MHCpro_dict = {'const': [1.0, 1.0],\n",
    "              'Age': [33.0, 33.0],\n",
    "              'CTRY_Germany': [1, 1],\n",
    "              'CTRY_United Kingdom': [0, 0],\n",
    "              'CTRY_United States': [0, 0],\n",
    "              'CTRY_Other': [0, 0],\n",
    "              'Gender_Male': [0.7, 0.7],\n",
    "              'Gender_Other': [0.01, 0.01],\n",
    "              'INT1_1': [total_df[total_df.CTRY == 'Germany']['INT1_1'].mean(), 0.9],\n",
    "              'INT1_2': [total_df[total_df.CTRY == 'Germany']['INT1_2'].mean(), 0.9],\n",
    "              'INT2_1': [total_df[total_df.CTRY == 'Germany']['INT2_1'].mean(), 0.1],\n",
    "              'INT3_1': [total_df[total_df.CTRY == 'Germany']['INT3_1'].mean(), 0.7],\n",
    "              'INT3_2': [total_df[total_df.CTRY == 'Germany']['INT3_2'].mean(), 0.67],\n",
    "              'INT3_3': [total_df[total_df.CTRY == 'Germany']['INT3_3'].mean(), 0.7],\n",
    "              'INT3_4': [total_df[total_df.CTRY == 'Germany']['INT3_4'].mean(), 0.7],\n",
    "              'INT1_4': [total_df[total_df.CTRY == 'Germany']['INT1_4'].mean(), 0.8] \n",
    "             }\n",
    "MHCpro_df = pd.DataFrame(MHCpro_dict)\n",
    "\n",
    "result_df = pd.DataFrame(columns=['Start', 'End', 'Diff'], \n",
    "                         index=['MHC level', 'Diagnosis level', 'Seaking streatment with D', 'Seaking streatment without D',\n",
    "                                'Productivity gain from T with D', 'Productivity gain from T without D', 'Total prod gain from T'])\n",
    "\n",
    "# Predicting MHCA change\n",
    "sdf = MHCpro_df[[*columns_mhca]]\n",
    "result_df.at['MHC level', 'Start'] = model_mhca.predict_proba(sdf)[:, 1][0] / 3.2     # Corecting survey self-selection bias on 3.2\n",
    "result_df.at['MHC level', 'End'] = model_mhca.predict_proba(sdf)[:, 1][1] / 3.2\n",
    "\n",
    "# Predicting MHCD change\n",
    "sdf = MHCpro_df[[*columns_mhcd]]\n",
    "result_df.at['Diagnosis level', 'Start'] = model_mhcd.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Diagnosis level', 'End'] = model_mhcd.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# Predicting ST change\n",
    "# for people with diagnosis\n",
    "MHCpro_df['MHCD'] = [1, 1]\n",
    "sdf = MHCpro_df[[*columns_st]]\n",
    "result_df.at['Seaking streatment with D', 'Start'] = model_st.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Seaking streatment with D', 'End'] = model_st.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# for people without diagnosis\n",
    "MHCpro_df['MHCD'] = [0, 0]\n",
    "sdf = MHCpro_df[[*columns_st]]\n",
    "result_df.at['Seaking streatment without D', 'Start'] = model_st.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Seaking streatment without D', 'End'] = model_st.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# Predicting ProD\n",
    "# for people with diagnosis\n",
    "MHCpro_df['MHCD'] = [1, 1]\n",
    "sdf = MHCpro_df[[*columns_ProDia]]\n",
    "result_df.at['Productivity gain from T with D', 'Start'] = model_ProDia.predict(sdf)[0] \n",
    "result_df.at['Productivity gain from T with D', 'End'] = model_ProDia.predict(sdf)[1]\n",
    "\n",
    "# for people without diagnosis\n",
    "MHCpro_df['MHCD'] = [0, 0]\n",
    "sdf = MHCpro_df[[*columns_ProNDia]]\n",
    "result_df.at['Productivity gain from T without D', 'Start'] = model_ProNDia.predict(sdf)[0] \n",
    "result_df.at['Productivity gain from T without D', 'End'] = model_ProNDia.predict(sdf)[1]\n",
    "\n",
    "# productivity gain for people with diagnosis\n",
    "result_df.at['Total prod gain from T', 'Start'] =  result_df.at['MHC level', 'Start'] * \\\n",
    "            (result_df.at['Diagnosis level', 'Start'] * result_df.at['Seaking streatment with D', 'Start'] * \\\n",
    "             result_df.at['Productivity gain from T with D', 'Start'] + \\\n",
    "             (1 - result_df.at['Diagnosis level', 'Start']) * result_df.at['Seaking streatment without D', 'Start'] *\\\n",
    "              result_df.at['Productivity gain from T without D', 'Start'])\n",
    "                                                           \n",
    "result_df.at['Total prod gain from T', 'End'] =  result_df.at['MHC level', 'End'] * \\\n",
    "            (result_df.at['Diagnosis level', 'End'] * result_df.at['Seaking streatment with D', 'End'] * \\\n",
    "             result_df.at['Productivity gain from T with D', 'End'] + \\\n",
    "             (1 - result_df.at['Diagnosis level', 'End']) * result_df.at['Seaking streatment without D', 'End'] *\\\n",
    "              result_df.at['Productivity gain from T without D', 'End'])\n",
    "\n",
    "result_df['Diff'] = result_df['End'] - result_df['Start']\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "15607fb3-cb84-45f6-87e4-c42195b56197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MHC level</th>\n",
       "      <td>0.154176</td>\n",
       "      <td>0.154176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diagnosis level</th>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaking streatment with D</th>\n",
       "      <td>0.877894</td>\n",
       "      <td>0.880324</td>\n",
       "      <td>0.00243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaking streatment without D</th>\n",
       "      <td>0.421881</td>\n",
       "      <td>0.427468</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Productivity gain from T with D</th>\n",
       "      <td>0.154239</td>\n",
       "      <td>0.158085</td>\n",
       "      <td>0.003845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Productivity gain from T without D</th>\n",
       "      <td>0.075697</td>\n",
       "      <td>0.08598</td>\n",
       "      <td>0.010283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total prod gain from T</th>\n",
       "      <td>0.014506</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Start       End      Diff\n",
       "MHC level                           0.154176  0.154176       0.0\n",
       "Diagnosis level                     0.600659  0.600659       0.0\n",
       "Seaking streatment with D           0.877894  0.880324   0.00243\n",
       "Seaking streatment without D        0.421881  0.427468  0.005587\n",
       "Productivity gain from T with D     0.154239  0.158085  0.003845\n",
       "Productivity gain from T without D  0.075697   0.08598  0.010283\n",
       "Total prod gain from T              0.014506  0.015151  0.000645"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df[['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4', 'INT1_4']] = \\\n",
    "        total_df[['INT1_1', 'INT1_2', 'INT2_1', 'INT3_1', 'INT3_2', 'INT3_3', 'INT3_4', 'INT1_4']].astype(float)\n",
    "MHCpro_dict = {'const': [1.0, 1.0],\n",
    "              'Age': [33.0, 33.0],\n",
    "              'CTRY_Germany': [1, 1],\n",
    "              'CTRY_United Kingdom': [0, 0],\n",
    "              'CTRY_United States': [0, 0],\n",
    "              'CTRY_Other': [0, 0],\n",
    "              'Gender_Male': [0.7, 0.7],\n",
    "              'Gender_Other': [0.01, 0.01],\n",
    "              'INT1_1': [total_df[total_df.CTRY == 'Germany']['INT1_1'].mean(), 0.0],\n",
    "              'INT1_2': [total_df[total_df.CTRY == 'Germany']['INT1_2'].mean(), 0.9],\n",
    "              'INT2_1': [total_df[total_df.CTRY == 'Germany']['INT2_1'].mean(), 0.1],\n",
    "              'INT3_1': [total_df[total_df.CTRY == 'Germany']['INT3_1'].mean(), 0.7],\n",
    "              'INT3_2': [total_df[total_df.CTRY == 'Germany']['INT3_2'].mean(), 0.67],\n",
    "              'INT3_3': [total_df[total_df.CTRY == 'Germany']['INT3_3'].mean(), 0.7],\n",
    "              'INT3_4': [total_df[total_df.CTRY == 'Germany']['INT3_4'].mean(), 0.7],\n",
    "              'INT1_4': [total_df[total_df.CTRY == 'Germany']['INT1_4'].mean(), 0.8] \n",
    "             }\n",
    "MHCpro_df = pd.DataFrame(MHCpro_dict)\n",
    "\n",
    "result_df = pd.DataFrame(columns=['Start', 'End', 'Diff'], \n",
    "                         index=['MHC level', 'Diagnosis level', 'Seaking streatment with D', 'Seaking streatment without D',\n",
    "                                'Productivity gain from T with D', 'Productivity gain from T without D', 'Total prod gain from T'])\n",
    "\n",
    "# Predicting MHCA change\n",
    "sdf = MHCpro_df[[*columns_mhca]]\n",
    "result_df.at['MHC level', 'Start'] = model_mhca.predict_proba(sdf)[:, 1][0] / 3.2     # Corecting survey self-selection bias on 3.2\n",
    "result_df.at['MHC level', 'End'] = model_mhca.predict_proba(sdf)[:, 1][1] / 3.2\n",
    "\n",
    "# Predicting MHCD change\n",
    "sdf = MHCpro_df[[*columns_mhcd]]\n",
    "result_df.at['Diagnosis level', 'Start'] = model_mhcd.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Diagnosis level', 'End'] = model_mhcd.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# Predicting ST change\n",
    "# for people with diagnosis\n",
    "MHCpro_df['MHCD'] = [1, 1]\n",
    "sdf = MHCpro_df[[*columns_st]]\n",
    "result_df.at['Seaking streatment with D', 'Start'] = model_st.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Seaking streatment with D', 'End'] = model_st.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# for people without diagnosis\n",
    "MHCpro_df['MHCD'] = [0, 0]\n",
    "sdf = MHCpro_df[[*columns_st]]\n",
    "result_df.at['Seaking streatment without D', 'Start'] = model_st.predict_proba(sdf)[:, 1][0] \n",
    "result_df.at['Seaking streatment without D', 'End'] = model_st.predict_proba(sdf)[:, 1][1]\n",
    "\n",
    "# Predicting ProD\n",
    "# for people with diagnosis\n",
    "MHCpro_df['MHCD'] = [1, 1]\n",
    "sdf = MHCpro_df[[*columns_ProDia]]\n",
    "result_df.at['Productivity gain from T with D', 'Start'] = model_ProDia.predict(sdf)[0] \n",
    "result_df.at['Productivity gain from T with D', 'End'] = model_ProDia.predict(sdf)[1]\n",
    "\n",
    "# for people without diagnosis\n",
    "MHCpro_df['MHCD'] = [0, 0]\n",
    "sdf = MHCpro_df[[*columns_ProNDia]]\n",
    "result_df.at['Productivity gain from T without D', 'Start'] = model_ProNDia.predict(sdf)[0] \n",
    "result_df.at['Productivity gain from T without D', 'End'] = model_ProNDia.predict(sdf)[1]\n",
    "\n",
    "# productivity gain for people with diagnosis\n",
    "result_df.at['Total prod gain from T', 'Start'] =  result_df.at['MHC level', 'Start'] * \\\n",
    "            (result_df.at['Diagnosis level', 'Start'] * result_df.at['Seaking streatment with D', 'Start'] * \\\n",
    "             result_df.at['Productivity gain from T with D', 'Start'] + \\\n",
    "             (1 - result_df.at['Diagnosis level', 'Start']) * result_df.at['Seaking streatment without D', 'Start'] *\\\n",
    "              result_df.at['Productivity gain from T without D', 'Start'])\n",
    "                                                           \n",
    "result_df.at['Total prod gain from T', 'End'] =  result_df.at['MHC level', 'End'] * \\\n",
    "            (result_df.at['Diagnosis level', 'End'] * result_df.at['Seaking streatment with D', 'End'] * \\\n",
    "             result_df.at['Productivity gain from T with D', 'End'] + \\\n",
    "             (1 - result_df.at['Diagnosis level', 'End']) * result_df.at['Seaking streatment without D', 'End'] *\\\n",
    "              result_df.at['Productivity gain from T without D', 'End'])\n",
    "\n",
    "result_df['Diff'] = result_df['End'] - result_df['Start']\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
